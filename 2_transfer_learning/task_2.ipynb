{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task-2-domain-transfer-WWW-2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX2V-RTlOCMp"
      },
      "source": [
        "# Conversational AI Workshop\n",
        "## Can your AI beat the Turing test?\n",
        "\n",
        "> This notebook will be used at the [Web Conference 2021](https://www2021.thewebconf.org) workshop [**Can your AI beat the Turing test?**](https://www2021.thewebconf.org/program/workshops-tutorials-april-12-16/#tab|4), taking place on April 16, 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_2pDHHFtwX5"
      },
      "source": [
        "# Task 2: Transfer learning\n",
        "In task 1, we learned about chatbots and trained our first language model.\n",
        "\n",
        "In this task, we will start with a language model that was pretrained on a large corpus of data from all kinds of sources to narrow it down to a domain and task of choice (a specific style for conversation generation).\n",
        "\n",
        "## Important resources\n",
        "* [Workshop Github repo](https://github.com/utanashati/conversational-ai-workshop)\n",
        "* [PyTorch documentation](https://pytorch.org/docs/stable/index.html)\n",
        "* Huggingface transformers library [ [Github](https://github.com/huggingface/transformers) | [Docs](https://huggingface.co/transformers/) ]\n",
        "\n",
        "# Approach\n",
        "To \"teach\" the model the structure of a conversation, we will utilize a naive approach of simply feeding the model \"raw\" conversations data of the form:\n",
        "```\n",
        "<speaker1> Hi\n",
        "<speaker2> Hey - how are you?\n",
        "<speaker1> Great, thanks!\n",
        "...\n",
        "```\n",
        "Our hope is that the model will learn this structure and we will  be able to query our model with an input of the form:\n",
        "\n",
        "```\n",
        "<speaker2> Am I speaking to a bot?\n",
        "<speaker1>\n",
        "```\n",
        "We then expect the model to extend the text from this prefix.\n",
        "\n",
        "In this notebook, you will go through all the steps fo this process, from collecting the training data to interacting with the final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUyajaGkR1JM"
      },
      "source": [
        "# Setting things up\n",
        "The following cells will clone the repository and install all the necessary dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC6KTnYXD3x4"
      },
      "source": [
        "!nvidia-smi | grep -q 'failed' && echo \"STOP! You are using a runtime without a GPU. Change the runtime type before going further!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5PGOly6SB12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "913e5694-f758-4cba-b40d-a699767d380b"
      },
      "source": [
        "!git clone https://github.com/utanashati/conversational-ai-workshop.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'artificial-self-AMLD-2020'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/92)\u001b[K\rremote: Counting objects:   2% (2/92)\u001b[K\rremote: Counting objects:   3% (3/92)\u001b[K\rremote: Counting objects:   4% (4/92)\u001b[K\rremote: Counting objects:   5% (5/92)\u001b[K\rremote: Counting objects:   6% (6/92)\u001b[K\rremote: Counting objects:   7% (7/92)\u001b[K\rremote: Counting objects:   8% (8/92)\u001b[K\rremote: Counting objects:   9% (9/92)\u001b[K\rremote: Counting objects:  10% (10/92)\u001b[K\rremote: Counting objects:  11% (11/92)\u001b[K\rremote: Counting objects:  13% (12/92)\u001b[K\rremote: Counting objects:  14% (13/92)\u001b[K\rremote: Counting objects:  15% (14/92)\u001b[K\rremote: Counting objects:  16% (15/92)\u001b[K\rremote: Counting objects:  17% (16/92)\u001b[K\rremote: Counting objects:  18% (17/92)\u001b[K\rremote: Counting objects:  19% (18/92)\u001b[K\rremote: Counting objects:  20% (19/92)\u001b[K\rremote: Counting objects:  21% (20/92)\u001b[K\rremote: Counting objects:  22% (21/92)\u001b[K\rremote: Counting objects:  23% (22/92)\u001b[K\rremote: Counting objects:  25% (23/92)\u001b[K\rremote: Counting objects:  26% (24/92)\u001b[K\rremote: Counting objects:  27% (25/92)\u001b[K\rremote: Counting objects:  28% (26/92)\u001b[K\rremote: Counting objects:  29% (27/92)\u001b[K\rremote: Counting objects:  30% (28/92)\u001b[K\rremote: Counting objects:  31% (29/92)\u001b[K\rremote: Counting objects:  32% (30/92)\u001b[K\rremote: Counting objects:  33% (31/92)\u001b[K\rremote: Counting objects:  34% (32/92)\u001b[K\rremote: Counting objects:  35% (33/92)\u001b[K\rremote: Counting objects:  36% (34/92)\u001b[K\rremote: Counting objects:  38% (35/92)\u001b[K\rremote: Counting objects:  39% (36/92)\u001b[K\rremote: Counting objects:  40% (37/92)\u001b[K\rremote: Counting objects:  41% (38/92)\u001b[K\rremote: Counting objects:  42% (39/92)\u001b[K\rremote: Counting objects:  43% (40/92)\u001b[K\rremote: Counting objects:  44% (41/92)\u001b[K\rremote: Counting objects:  45% (42/92)\u001b[K\rremote: Counting objects:  46% (43/92)\u001b[K\rremote: Counting objects:  47% (44/92)\u001b[K\rremote: Counting objects:  48% (45/92)\u001b[K\rremote: Counting objects:  50% (46/92)\u001b[K\rremote: Counting objects:  51% (47/92)\u001b[K\rremote: Counting objects:  52% (48/92)\u001b[K\rremote: Counting objects:  53% (49/92)\u001b[K\rremote: Counting objects:  54% (50/92)\u001b[K\rremote: Counting objects:  55% (51/92)\u001b[K\rremote: Counting objects:  56% (52/92)\u001b[K\rremote: Counting objects:  57% (53/92)\u001b[K\rremote: Counting objects:  58% (54/92)\u001b[K\rremote: Counting objects:  59% (55/92)\u001b[K\rremote: Counting objects:  60% (56/92)\u001b[K\rremote: Counting objects:  61% (57/92)\u001b[K\rremote: Counting objects:  63% (58/92)\u001b[K\rremote: Counting objects:  64% (59/92)\u001b[K\rremote: Counting objects:  65% (60/92)\u001b[K\rremote: Counting objects:  66% (61/92)\u001b[K\rremote: Counting objects:  67% (62/92)\u001b[K\rremote: Counting objects:  68% (63/92)\u001b[K\rremote: Counting objects:  69% (64/92)\u001b[K\rremote: Counting objects:  70% (65/92)\u001b[K\rremote: Counting objects:  71% (66/92)\u001b[K\rremote: Counting objects:  72% (67/92)\u001b[K\rremote: Counting objects:  73% (68/92)\u001b[K\rremote: Counting objects:  75% (69/92)\u001b[K\rremote: Counting objects:  76% (70/92)\u001b[K\rremote: Counting objects:  77% (71/92)\u001b[K\rremote: Counting objects:  78% (72/92)\u001b[K\rremote: Counting objects:  79% (73/92)\u001b[K\rremote: Counting objects:  80% (74/92)\u001b[K\rremote: Counting objects:  81% (75/92)\u001b[K\rremote: Counting objects:  82% (76/92)\u001b[K\rremote: Counting objects:  83% (77/92)\u001b[K\rremote: Counting objects:  84% (78/92)\u001b[K\rremote: Counting objects:  85% (79/92)\u001b[K\rremote: Counting objects:  86% (80/92)\u001b[K\rremote: Counting objects:  88% (81/92)\u001b[K\rremote: Counting objects:  89% (82/92)\u001b[K\rremote: Counting objects:  90% (83/92)\u001b[K\rremote: Counting objects:  91% (84/92)\u001b[K\rremote: Counting objects:  92% (85/92)\u001b[K\rremote: Counting objects:  93% (86/92)\u001b[K\rremote: Counting objects:  94% (87/92)\u001b[K\rremote: Counting objects:  95% (88/92)\u001b[K\rremote: Counting objects:  96% (89/92)\u001b[K\rremote: Counting objects:  97% (90/92)\u001b[K\rremote: Counting objects:  98% (91/92)\u001b[K\rremote: Counting objects: 100% (92/92)\u001b[K\rremote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 378 (delta 44), reused 33 (delta 12), pack-reused 286\u001b[K\n",
            "Receiving objects: 100% (378/378), 43.24 MiB | 27.08 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8LoBjs4SOhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af128d92-8c81-47a6-ed2e-220f5750f614"
      },
      "source": [
        "# Set working directory\n",
        "%cd /content/conversational-ai-workshop/2_transfer_learning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/artificial-self-AMLD-2020/2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2e-ZKZISylw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "5c5fdc23-a4dd-4697-9835-ec32bb8b7042"
      },
      "source": [
        "# Install all dependencies for this task\n",
        "!pip install -r requirements-colab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 25.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 2.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements-colab.txt (line 1)) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements-colab.txt (line 1)) (1.10.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements-colab.txt (line 1)) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 15.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements-colab.txt (line 1)) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements-colab.txt (line 1)) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements-colab.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements-colab.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements-colab.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements-colab.txt (line 1)) (2019.11.28)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements-colab.txt (line 1)) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements-colab.txt (line 1)) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->-r requirements-colab.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r requirements-colab.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r requirements-colab.txt (line 1)) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r requirements-colab.txt (line 1)) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers==2.3.0->-r requirements-colab.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers==2.3.0->-r requirements-colab.txt (line 1)) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=fd16435cdfd0cc0189d2a9c76b907384d22371c8062c587fa853b4f8c36a1b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abw4P7xTJn6W"
      },
      "source": [
        "# Mount Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPzEnG_tlL3I"
      },
      "source": [
        "# Import training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAE4MrVSVmV1"
      },
      "source": [
        "## Dataset 1: World leader interviews\n",
        "\n",
        "The first option is a dataset of interviews of two world leaders, Barack Obama and Vladimir Putin. These interviews will be treated as chat conversations, where the reporters are the interlocutors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9PmICPoWt98"
      },
      "source": [
        "# Barack Obama\n",
        "# input_data = 'datasets/barack_obama_interviews.json'\n",
        "\n",
        "# Vladimir Putin\n",
        "# input_data = 'datasets/vladimir_putin_interviews.json'\n",
        "\n",
        "assert os.path.isfile(input_data), \"File not found\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQCXoLLhO2FG"
      },
      "source": [
        "## Dataset 2: Movie quotes\n",
        "\n",
        "Another option is to use quotes from movies. You can use the [Cornell Movie-Dialogs Corpus](https://), which contains **220,579 conversational exchanges** between 10,292 pairs of movie characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRYuNd85O5cl"
      },
      "source": [
        "# input_data = 'datasets/cornell_movie_dialogs_corpus.json.zip'\n",
        "\n",
        "assert os.path.isfile(input_data), \"File not found\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxNG4fmqcHGt"
      },
      "source": [
        "# Prepare the data\n",
        "For this task we will use the transfomers library by Huggingface. The transformers library implements many recent NLP models (such as BERT and GPT-2).\n",
        "\n",
        "Our data is currently in JSON format, but can easily be read into a Pandas Dataframe.\n",
        "\n",
        "As a first step we want to get our conversation data from this format:\n",
        "\n",
        "| timestamp | conversationId | conversationWithName | senderName | outgoing | text | language | platform |\n",
        "| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n",
        "| 1575463019 | 693342290 | Alice | Bob | True | Hi Alice! | en | whatsapp |\n",
        "| 1575463025 | 693342290 | Alice | Alice | False | Hi Bob! How are you these days? | en | whatsapp |\n",
        "| 1575463030 | 693342290 | Alice | Bob | True | Great! Thanks | en | whatsapp |\n",
        "\n",
        "and get into a text file of this format:\n",
        "```\n",
        "<person1> Hi Alice!\n",
        "<person2> Hi Bob! How are you these days?\n",
        "<person1> Great! Thanks\n",
        "...\n",
        "```\n",
        "\n",
        "Our model will try to generate this structure from the data. The tags `<person1>` and `<person2>` have nothing special to them and could in theory be replaced by something else as well (more about this below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLHMIoUbTFp-"
      },
      "source": [
        "from utils import generate_input_task2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvdcn7HBTGgG"
      },
      "source": [
        "assert os.path.isfile(input_data), 'Input file not found'\n",
        "generate_input_task2(input_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00KS5e8KYpWz"
      },
      "source": [
        "The script has now generated an input file `cached_input_task2.txt`. You can inspect it with the `head` command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUh1_4LBpA05"
      },
      "source": [
        "!head cached_input_task2.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9gLQPF6Y-K4"
      },
      "source": [
        "# Train the model\n",
        "We can now start training the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GsvtlOlZSAF"
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import chain\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    GPT2Config,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    OpenAIGPTConfig,\n",
        "    OpenAIGPTLMHeadModel,\n",
        "    OpenAIGPTTokenizer,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from utils import get_input_task2, set_seed, add_special_tokens_\n",
        "import logging\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)-5.5s] [%(name)-12.12s]: %(message)s')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYeG8QI0ce_W"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckAH0GK5aH0t"
      },
      "source": [
        "run_name = 'run1'               # The name of the run (subdirectory in ./runs)\n",
        "model_type = 'openai-gpt'       # Initialize model from path to checkpoint or with model name (\"openai-gpt\" or \"gpt2\")\n",
        "save_every = 50                 # Save checkpoint every n updates steps.\n",
        "max_input_length = 400          # Number of tokens which will be fed into the model (reduce this number if you have memory constraints)\n",
        "weight_decay = 0                # Weight decay if we apply some.\n",
        "train_batch_size = 8            # Batch size for training\n",
        "gradient_accumulation_steps = 8 # Accumulate gradients on several steps\n",
        "lr = 5e-5                       # Learning rate\n",
        "adam_epsilon = 1e-8             # Epsilon for Adam optimizer.\n",
        "max_norm = 1                    # Clipping gradient norm\n",
        "n_epochs = 2                    # Number of training epochs\n",
        "device = 'cuda'                 # Device (cuda or cpu)\n",
        "warmup_steps = 0                # Linear warmup over warmup_steps.\n",
        "seed = 42                       # random seed for initialization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFnOq8n4cib3"
      },
      "source": [
        "### Data loading\n",
        "In PyTorch, we can define a custom class that inherits from the Dataset class to control the loading of our data. In this class, we will override the `__init__()` initializer to\n",
        "\n",
        "1. Read the text file which we generated before\n",
        "2. Tokenize the text (split it into smaller words/character pairs) and convert the tokens into vocabulary IDs (the positions of the tokens in the vocabulary. GPT-2 uses so-called BPE (byte-pair encoding). If you're interested how it works you can read more about it in [this blog post](https://leimao.github.io/blog/Byte-Pair-Encoding/).\n",
        "3. Cut the array of token IDs into chunks of size `max_input_length` (usually chosen to the maximum of what the memory/model allows)\n",
        "4. Append the generated training example as a list\n",
        "\n",
        "The `__get_item__()` simply implements the retrieval of a new training example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeBZosNKZYUa"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer):\n",
        "        # load the text data generated from before into memory\n",
        "        text = get_input_task2(input_data)\n",
        "        logger.info(\"Tokenizing and building input...\")\n",
        "        # tokenize the whole file\n",
        "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "        # generate training examples by cutting the text into blocks of size max_input_length\n",
        "        self.examples = []\n",
        "        block_size = max_input_length\n",
        "        if block_size < 0:\n",
        "            # use maximum possible input block size\n",
        "            block_size = tokenizer.max_len_single_sentence\n",
        "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])\n",
        "\n",
        "def get_data_loader(tokenizer):\n",
        "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
        "    dataset = TextDataset(tokenizer)\n",
        "    logger.info(\"Train dataset: {:,} samples\".format(len(dataset)))\n",
        "    logger.info(\"Build dataloaders\")\n",
        "    data_loader = DataLoader(dataset, batch_size=train_batch_size, shuffle=True)\n",
        "    return data_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7voPNvpgjL2"
      },
      "source": [
        "# Setting the same seed allows for some reproducibility of the experiments\n",
        "set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXbo_iLqiEjf"
      },
      "source": [
        "### Load models and tokenizers\n",
        "The transformers library comes with a built in method `from_pretrained(model_type)`. `model_type` can specify either\n",
        "1. One of the [pretrained model architectures](https://huggingface.co/transformers/pretrained_models.html). In this case the model will be downloaded and cached on disk before loaded into memory.\n",
        "2. The path to a folder with an existing model checkpoint.\n",
        "\n",
        "This allows us to use the same syntax to both pretrained and fine-tuned models/tokenizers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QddFgKLlZlHI"
      },
      "source": [
        " # Load tokenizer\n",
        " logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
        " tokenizer_class = GPT2Tokenizer if \"gpt2\" in model_type else OpenAIGPTTokenizer\n",
        " tokenizer = tokenizer_class.from_pretrained(model_type)\n",
        " # Load model\n",
        " model_class = GPT2LMHeadModel if \"gpt2\" in model_type else OpenAIGPTLMHeadModel\n",
        " model = model_class.from_pretrained(model_type)\n",
        " model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsD0MYlajkFE"
      },
      "source": [
        "### Add special tokens\n",
        "Let's see how `<speaker1>` and `<speaker2>` tokens will be tokenized by our current tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s0G97YYjz1D"
      },
      "source": [
        "tokenizer.tokenize('<speaker1>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7YQdj8ikKKA"
      },
      "source": [
        "As you can see, the model generates a total of 4 tokens: `['<', 'speaker', '1', '>']`. This doesn't make too much sense for us since the tags should not contain any meaning but should simply indicate who is currently speaking. Luckily, there is an easy way to add our speaker tokens to the vocabulary of the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o11KdvWNjWik"
      },
      "source": [
        "ATTR_TO_SPECIAL_TOKEN = {'additional_special_tokens': ('<speaker1>', '<speaker2>')}\n",
        "# Add special tokens if they are not already added\n",
        "def add_special_tokens_(model, tokenizer):\n",
        "   \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"                                                                                   \n",
        "   orig_num_tokens = len(tokenizer.encoder)\n",
        "   num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there                                                                   \n",
        "   if num_added_tokens > 0:\n",
        "       model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
        "       \n",
        "add_special_tokens_(model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJOfCIAblBH1"
      },
      "source": [
        "Now the model should generate a single token:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk0ehp_KlDYn"
      },
      "source": [
        "tokenizer.tokenize('<speaker1>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7hnYlQklwmE"
      },
      "source": [
        "# Final setup before training\n",
        "We need to set up a few things before we can start training:\n",
        "* Prepare the data loaders (discussed above)\n",
        "* An optimizer (we will use Adam)\n",
        "* A scheduler to change the learning rate throughout training (we will use a [linear schedule with warmup](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdj6wabyjQ0V"
      },
      "source": [
        " # Get data loaders\n",
        " logger.info(\"Prepare datasets\")\n",
        " data_loader = get_data_loader(tokenizer)\n",
        " # Prepare optimizer and schedule (linear warmup and decay)\n",
        " no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        " optimizer_grouped_parameters = [\n",
        "     {\n",
        "         \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         \"weight_decay\": weight_decay,\n",
        "     },\n",
        "     {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        " ]\n",
        " optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
        " t_total = len(data_loader) // gradient_accumulation_steps * n_epochs\n",
        " scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmaaiel7OGTg"
      },
      "source": [
        "Check how long the training will take. Make sure this is below ~20min if possible (e.g. by reducing `n_epochs`). However, you can always stop the training and then refer to one of the saved checkpoints as `{run_name}/checkpoint-X`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogff9tI2N5O5"
      },
      "source": [
        "print(f\"Your training time is approx. {len(data_loader)*n_epochs/((8/train_batch_size)*60):.0f} min\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TMwH4HnnLGw"
      },
      "source": [
        "### Training\n",
        "\n",
        "Finally we can start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOu849kknPnz"
      },
      "source": [
        "logger.info(\"***** Running training *****\")\n",
        "global_step = 0\n",
        "epochs_trained = 0\n",
        "steps_trained_in_current_epoch = 0\n",
        "# Check if we are training from a checkpoint or from a pretrained model\n",
        "if os.path.exists(model_type):\n",
        "    # set global_step to gobal_step of last saved checkpoint from model path\n",
        "    global_step = int(model_type.split(\"-\")[-1].split(\"/\")[0])\n",
        "    epochs_trained = global_step // (len(data_loader) // gradient_accumulation_steps)\n",
        "    steps_trained_in_current_epoch = global_step % (len(data_loader) // gradient_accumulation_steps)\n",
        "    logger.info(\"Continuing training from checkpoint, will skip to saved global_step\")\n",
        "    logger.info(f\"Continuing training from epoch {epochs_trained}\")\n",
        "    logger.info(f\"Continuing training from global step {global_step}\")\n",
        "    logger.info(f\"Will skip the first {steps_trained_in_current_epoch} steps in the first epoch\")\n",
        "\n",
        "# Training loop\n",
        "model.zero_grad()\n",
        "epoch_pbar = trange(epochs_trained, int(n_epochs)) # epoch progress bar\n",
        "av_loss = 0\n",
        "for current_epoch in epoch_pbar:\n",
        "    epoch_pbar.set_description(f\"Epoch [{current_epoch+1}/{n_epochs}]\") # description of epoch progress bar\n",
        "    pbar = tqdm(data_loader, position=0) # progress bar\n",
        "    for step, batch in enumerate(pbar):\n",
        "        # Skip past any already trained steps if resuming training\n",
        "        if steps_trained_in_current_epoch > 0:\n",
        "            steps_trained_in_current_epoch -= 1\n",
        "            continue\n",
        "        model.train()\n",
        "        # the language model targets (labels) are the same as the input!\n",
        "        inputs, labels = (batch, batch)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        loss, *_ = model(inputs, labels=labels)\n",
        "        loss.backward()\n",
        "        tr_loss = loss.item()\n",
        "        # Compute a running average of the loss\n",
        "        av_loss = (step*av_loss + tr_loss)/(step + 1)\n",
        "        pbar.set_description(f\"Average loss: {av_loss:.4f}\")\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "            if global_step % save_every == 0 and global_step > 0:\n",
        "                checkpoint_prefix = \"checkpoint\"\n",
        "                output_dir = os.path.join('runs', run_name, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                if not os.path.exists(output_dir):\n",
        "                    os.makedirs(output_dir)\n",
        "                logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
        "                model.save_pretrained(output_dir)\n",
        "                tokenizer.save_pretrained(output_dir)\n",
        "                logger.info(f\"Saving optimizer and scheduler states to {output_dir}\")\n",
        "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "\n",
        "# save model\n",
        "output_dir = os.path.join('runs', run_name)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ew8SGqBB-Te"
      },
      "source": [
        "# (Optional) Tweaking parameters\n",
        "\n",
        "You can change the training parameters to see how they affect the language model. For this, adjust them below, then run the again the cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRyEvCxPB_Np"
      },
      "source": [
        "run_name = 'run1'               # The name of the run (subdirectory in ./runs)\")\n",
        "model_type = 'openai-gpt'       # Initialize model from path to checkpoint or with model name (openai-gpt/openai-gpt2)\"\n",
        "weight_decay = 0                # Weight decay if we apply some.\n",
        "train_batch_size = 4            # Batch size for training\n",
        "gradient_accumulation_steps = 8 # Accumulate gradients on several steps\n",
        "lr = 5e-5                       # Learning rate\n",
        "n_epochs = 1                    # Number of training epochs\n",
        "warmup_steps = 0                # Linear warmup over warmup_steps."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v4Y2p3RpOBB"
      },
      "source": [
        "# Interact with the model\n",
        "The trained model can now be found under `./runs/{run_name}/`.\n",
        "\n",
        "Let's see what happens when we feed our model some text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiT2mrSAqvWS"
      },
      "source": [
        "input_ids = torch.tensor(tokenizer.encode(\"Hello world\", add_special_tokens=True), device=device).unsqueeze(0)\n",
        "print('Input IDs:')\n",
        "print(input_ids)\n",
        "out, = model(input_ids)\n",
        "print('Model output:')\n",
        "print(out)\n",
        "print('Output shape:')\n",
        "print(out.shape) # output shape: (batch size x sequence length x hidden size)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuP7ua6Jt_WO"
      },
      "source": [
        "As you can see, the output tensor size equals the number of the input tokens. By getting the highest value of its last dimension, we get the most likely next token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO7VgD0sqxzu"
      },
      "source": [
        "tokenizer.decode([torch.argmax(out[:, 1, :])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHx8BTrvVsW"
      },
      "source": [
        "## Start chatting\n",
        "As we have seen in task 1, we have several hyperparameters to choose from in order to control how we sample from the output probability distribution. Simply choosing the most likely token will not lead to interesting conversations.\n",
        "\n",
        "Below you can find a couple of more sophisticated sampling strategies. By default we will use top-p sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5UrWZw6u1M7"
      },
      "source": [
        "# Constants\n",
        " max_history = 2                  # Number of previous utterances to keep in history\n",
        " no_sample = False                # Set to use greedy decoding instead of sampling\n",
        " max_length = 80                  # Maximum length of the output utterances\n",
        " temperature = 1.0                # Sampling softmax temperature\n",
        " top_k = 0                        # Filter top-k tokens before sampling (<=0: no filtering)\n",
        " top_p = 0.8                      # Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\n",
        " no_info = False                   # Only show conversation output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaL732AIxCYl"
      },
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size x vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "def sample_sequence(conversation, model, num_samples=1):\n",
        "    \"\"\"Generate next tokens from pervious conversation\"\"\"\n",
        "    context = torch.tensor(conversation, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            inputs = {'input_ids': generated}\n",
        "            outputs = model(**inputs)\n",
        "            # scale by temperature\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.) \n",
        "            # filter by top-k/top-p\n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # greedy sampling:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBpALwQAyWb9"
      },
      "source": [
        "We are ready to interact with the model. A few things to note:\n",
        "* We will give it a \"trigger\" to start the conversation. From there the model takes it. Note that the model is \"playing\" speaker1 and you are speaker2. \n",
        "* If the `no_info` flag is set to `False`, the output shows both the input (conversation history) as well as the full output of the model. At the very end the answer which was selected by the model is shown.\n",
        "* You can press `h` (and then ENTER) in order to see the whole history of the chat\n",
        "\n",
        "\n",
        "Enjoy! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4gwwMEoxwAd"
      },
      "source": [
        "history = []\n",
        "speaker1_tag = '<speaker1>'\n",
        "speaker2_tag = '<speaker2>'\n",
        "speaker1_tag_id = tokenizer.convert_tokens_to_ids(speaker1_tag)\n",
        "speaker2_tag_id = tokenizer.convert_tokens_to_ids(speaker2_tag)\n",
        "history = f\"\"\"\n",
        "{speaker2_tag} Hi!\n",
        "{speaker1_tag} Hello\n",
        "{speaker2_tag} Are you ready?\n",
        "{speaker1_tag} Yes!\n",
        "{speaker2_tag} Ok let's start chatting\n",
        "{speaker1_tag} Sure, what do you want to talk about?\"\"\"\n",
        "print(history)\n",
        "print('\\n[Chat with the model! Send \"h\" to see the full history]\\n')\n",
        "history = history.split('\\n')\n",
        "while True: \n",
        "    message = None\n",
        "    while not message:\n",
        "        message = input(f'{speaker2_tag} ')\n",
        "        if message == 'h':\n",
        "            print('\\n'.join(history))\n",
        "            message = None\n",
        "    # add new message to history\n",
        "    history.append(f'{speaker2_tag} {message}')\n",
        "    # keep only most recent conversation as input to the model\n",
        "    recent_history = history[-(2*max_history):]\n",
        "    # concatenate history into single string and add trigger word \"bot:\"\n",
        "    history_str = '{}\\n{}'.format('\\n'.join(recent_history), speaker1_tag)\n",
        "    # tokenize text and convert into vocabulary ids (input ids)\n",
        "    history_enc = tokenizer.encode(history_str, add_special_tokens=True)\n",
        "    with torch.no_grad():\n",
        "        out_ids = sample_sequence(history_enc, model)\n",
        "    out_ids = out_ids[:, len(history_enc):].tolist()[0]\n",
        "    if not no_info:\n",
        "        print(20*'-')\n",
        "        print('Output of model:')\n",
        "        full_output = tokenizer.decode(out_ids, clean_up_tokenization_spaces=True)\n",
        "        print(full_output)\n",
        "        print('\\nInput to the model:')\n",
        "        print(history_str)\n",
        "        print(20*'-' + '\\n')\n",
        "    # Select part before speaker tags as answer\n",
        "    for i, out_id in enumerate(out_ids):\n",
        "        if out_id in [speaker1_tag_id, speaker2_tag_id]:\n",
        "            break\n",
        "    answer = '{} {}'.format(speaker1_tag, tokenizer.decode(out_ids[:i]))\n",
        "    print(answer)\n",
        "    # add answer to history\n",
        "    history.append(answer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkyH26kEB2Su"
      },
      "source": [
        "You can now review this whole conversation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNY97GdPB1hn"
      },
      "source": [
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoqHoD_hW1PW"
      },
      "source": [
        "### Save model to Google Drive\n",
        "If you are happy with your model consider saving it to your Google Drive. Note that all data on this notebook will be lost after a certain time of inactivity. Note that the model size is quite big (~500MB) so make sure you have enough space in your Google Drive.\n",
        "\n",
        "This will save only your final model state (from your directory `run_name` directory).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_NMpXM2W17W"
      },
      "source": [
        "import shutil\n",
        "\n",
        "#@title Save to Google Drive\n",
        "save_to_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "source_directory = f'./runs/{run_name}'\n",
        "target_directory = f\"/content/drive/My Drive/AMLD/models/task2/{run_name}/\"\n",
        "include_checkpoints = False\n",
        "\n",
        "if save_to_drive:\n",
        "  logger.info(f'Copying from {source_directory} to {target_directory}...')\n",
        "  ignore_pattern = None\n",
        "  if not include_checkpoints:\n",
        "    ignore_pattern = shutil.ignore_patterns('checkpoint-*')\n",
        "  shutil.copytree(source_directory, target_directory, ignore=ignore_pattern)\n",
        "  logger.info('Successfully copied your model!')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B6F1B87W6y8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}